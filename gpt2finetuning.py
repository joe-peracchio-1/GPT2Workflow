# -*- coding: utf-8 -*-
"""GPT2Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136XValHUAIIAqQzXTT-NwbwoaRsgxU86
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
nltk.download('punkt')

def remove_stop_words(input_file, output_file):
    # Read the input file
    with open(input_file, 'r', encoding='utf-8') as file:
        text = file.read()

    # Tokenize the text
    words = nltk.word_tokenize(text)

    # Remove stop words and punctuation
    stop_words = set(stopwords.words('english'))
    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]

    # Write the result to the output file
    with open(output_file, 'w', encoding='utf-8') as file:
        file.write(' '.join(filtered_words))

# Replace 'input.txt' and 'output.txt' with your file names
remove_stop_words('politic.txt', 'politicclean.txt')

def cleaning(s):
    s = str(s)
    s = re.sub('\s\W',' ',s)
    s = re.sub('\W,\s',' ',s)
    s = re.sub("\d+", "", s)
    s = re.sub('\s+',' ',s)
    s = re.sub('[!@#$_]', '', s)
    s = s.replace("co","")
    s = s.replace("https","")
    s = s.replace("[\w*"," ")
    return s

!pip install transformers

from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import Trainer, TrainingArguments

def load_dataset(file_path, tokenizer, block_size = 128):
    dataset = TextDataset(
        tokenizer = tokenizer,
        file_path = file_path,
        block_size = block_size,
    )
    return dataset


def load_data_collator(tokenizer, mlm = False):
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=mlm,
    )
    return data_collator

def train(train_file_path,model_name,
          output_dir,
          overwrite_output_dir,
          per_device_train_batch_size,
          num_train_epochs,
          save_steps):
  tokenizer = GPT2Tokenizer.from_pretrained(model_name)
  train_dataset = load_dataset(train_file_path, tokenizer)
  data_collator = load_data_collator(tokenizer)

  tokenizer.save_pretrained(output_dir)

  model = GPT2LMHeadModel.from_pretrained(model_name)

  model.save_pretrained(output_dir)

  training_args = TrainingArguments(
          output_dir=output_dir,
          overwrite_output_dir=overwrite_output_dir,
          per_device_train_batch_size=per_device_train_batch_size,
          num_train_epochs=num_train_epochs,
      )
  trainer = Trainer(
          model=model,
          args=training_args,
          data_collator=data_collator,
          train_dataset=train_dataset,
  )

  trainer.train()
  trainer.save_model()

train_file_path = "/content/gpt2train.txt"
model_name = 'gpt2'
output_dir = '/content/result'
overwrite_output_dir = False
per_device_train_batch_size = 8
num_train_epochs = 3.0
save_steps = 500



!pip uninstall accelerate
!pip install accelerate

train(
    train_file_path=train_file_path,
    model_name=model_name,
    output_dir=output_dir,
    overwrite_output_dir=overwrite_output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    num_train_epochs=num_train_epochs,
    save_steps=save_steps
)

def load_model(model_path):
    model = GPT2LMHeadModel.from_pretrained(model_path)
    return model


def load_tokenizer(tokenizer_path):
    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)
    return tokenizer

def generate_text(sequence, max_length):
    model_path = "/content/result"
    model = load_model(model_path)
    tokenizer = load_tokenizer(model_path)
    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')
    final_outputs = model.generate(
        ids,
        do_sample=True,
        max_length=max_length,
        pad_token_id=model.config.eos_token_id,
        top_k=50,
        top_p=0.95,
    )
    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))

sequence = "What does it mean to be a constructivist?"
max_len = 300 # 20
print(generate_text(sequence, max_len))

sequence = "What would a constructivist say to this: In the Ukrainian conflict, the phases of warfare have seen shifts in decisive weapons. Initially, the NLAW bazooka and later Russian artillery played key roles. Currently, the focus is on the US-made Himars rocket artillery, aiding Ukraine in impeding the Russian advance, particularly in Kherson. Himars' longer range has boosted Ukrainian confidence. However, challenges remain in capturing Kherson without causing excessive damage, and the overall military situation is in equilibrium despite Western supplies arriving gradually."
max_len = 500 # 20
print(generate_text(sequence, max_len))

sequence = "How would a constructivist respond to this? German consumers may face a tripling of gas prices in the coming months due to Russia's reduction of gas flow through the Nord Stream 1 pipeline by 40%, leading to a four- to sixfold rise in market prices, according to Klaus Müller, head of Germany's federal network agency. The German economic ministry has entered the second phase of an energy emergency plan, warning of a high risk of long-term supply shortages, and officials express concerns about a potential complete stop to Russian gas deliveries after the Nord Stream 1 annual inspection in July."
max_len = 500 # 20
print(generate_text(sequence, max_len))

sequence = "How would a constructivist respond to this? Vladimir Putin and Gazprom are reducing gas supplies to Europe, raising concerns that Russia might cut off all gas exports this winter. With Russia supplying 40% of Europe's gas before the war, countries are struggling to replace it, resorting to early storage filling, increased LNG imports from the US, and seeking alternatives from Norway, Azerbaijan, and renewable sources. If replacements fall short, businesses may limit energy use, causing potential economic challenges, especially for energy-intensive industries in countries like Germany. The UK, importing only 4% from Russia, is less affected, but prices are expected to rise, with potential disruptions and higher energy bills for consumers across Europe."
max_len = 500 # 20
print(generate_text(sequence, max_len))

sequence = "How would a constructivist respond to this? The US has announced a new $550 million weapons package for Ukraine, including ammunition for rocket launchers and artillery guns, while reports suggest that Russia has claimed to destroy six US-made Himars missile systems. In other developments, three people were reportedly killed by Russian shelling during an evacuation near Kherson, Ukraine, and Turkey's representative at the Joint Coordination Centre in Istanbul announced the arrival of the first ship carrying Ukrainian grain to world markets. Meanwhile, Ukraine's state security service is investigating 752 cases of treason and collaboration, and UN Secretary-General António Guterres warned of the risk of nuclear destruction amid escalating tensions. Sabina Higgins, the wife of Ireland's president, sparked controversy by urging a ceasefire and negotiations, with critics accusing her intervention of potentially promoting Kremlin propaganda."
max_len = 500 # 20
print(generate_text(sequence, max_len))

sequence = "How would a constructivist respond to this? Ukraine aims to assemble a 'million strong' army to potentially retake Russian-occupied territory, signaling a desire for Western support. However, doubts persist about the effectiveness of a counteroffensive, considering the significant casualties suffered by Ukrainian forces in the ongoing conflict in Donbas and the need to replenish and enhance the fighting force. While Western support, including training and weaponry, is underway, the success of any counteroffensive hinges on strategic planning, concentration of forces, and advanced weaponry, and the timeline for a potential military turnaround remains uncertain."
max_len = 500 # 20
print(generate_text(sequence, max_len))

sequence = "How would a constructivist respond to this? Russia has prohibited 39 prominent British figures, including Labour party leader Keir Starmer, former Prime Minister David Cameron, and presenter Piers Morgan, from entering the country. The Russian foreign ministry cited their alleged contribution to London's 'hostile course' aiming to demonize and isolate Russia, with plans to expand the 'stop list' due to London's perceived sanctions. The list also includes Shadow Foreign Secretary David Lammy, Shadow Levelling Up Secretary Lisa Nandy, and journalists from outlets such as The Guardian, BBC, Sunday Times, Economist, Daily Telegraph, and Sky News. This move follows a broader crackdown on Russian and foreign independent news outlets since the invasion of Ukraine in February, with over 200 Britons, including politicians, already banned from entering Russia. Additionally, Russia has criminalized media outlets spreading 'false information' about its army, leading to the closure of media groups and blocking access to foreign news organizations' websites, while designating the British-based Calvert 22 cultural foundation as an 'undesirable organization.'"
max_len = 500 # 20
print(generate_text(sequence, max_len))

